{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733ed7a9",
   "metadata": {},
   "source": [
    "# (5). ELU(Exponantial Linear Unit)\n",
    "\n",
    "The Exponential Linear Unit (ELU) activation function is a variant of the Rectified Linear Unit (ReLU) activation function that introduces a negative saturation regime to handle negative inputs. It aims to address the limitations of the ReLU function, such as the \"dying ReLU\" problem and the lack of activation for nagative inputs\n",
    "\n",
    "The ELU activation function is defined as follows:\n",
    "\n",
    "ELU(x) = x if x >= 0, alpha* (exp(x) - 1) if x < 0.\n",
    "\n",
    "where 'x' is the input value and alpha is a hypreparameter that controls the finctio's behaviour for naegative inputs.\n",
    "\n",
    "The ELU activation function provides the following advantages:\n",
    "\n",
    "(1) Non-zero gradient for negative inputs: Unlike ReLU, which assigns zero gradients for negative inputs, ELU assigns non-zero gradients. This helps address the \"dying ReLU\" problem, where neurons can become inactive and stop learning due to always receiving negative inputs. The non-zero gradients for negative inputs allow neurons to recover and continue learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9660bbc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58 (232.00 Byte)\n",
      "Trainable params: 58 (232.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "predicted outpur:[[0.9041963  0.11634971]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#define input data\n",
    "input_data=[[1.0,2.0,-3.0,4.0]]\n",
    "target_data=[[1.0,2.0]]    #Adjust target data shape to (1,2)\n",
    "\n",
    "#create a sequential model\n",
    "model=tf.keras.Sequential()\n",
    "\n",
    "#add a dense layer with ELU activation function\n",
    "model.add(tf.keras.layers.Dense(8,activation=tf.nn.elu,input_shape=(4,)))\n",
    "\n",
    "#Add output layer\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "#print model summary\n",
    "model.summary()\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "#train the model\n",
    "model.fit(input_data,target_data,epochs=1000,verbose=0)\n",
    "\n",
    "#test the model\n",
    "test_input=[[1.0,2.0,3.0,4.0]]\n",
    "predicted_output=model.predict(test_input)\n",
    "\n",
    "print(f\"predicted outpur:{predicted_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2af6f",
   "metadata": {},
   "source": [
    "# 7. SoftMax Function\n",
    "\n",
    "The softmax function is primarily used in the output layer of a neural network for multi-class classification problems. It provides a convenient way to interpret the outputs as probabilities and make predictions based on the highest probability class. The most common loss function used with softmax is the categorical cross-entropy loss, which measures the difference between the predicted probabilities and the true class labels.\n",
    "\n",
    "The softmax activation function is a commonly used activation function in the output layer of a neural network, particularly for multi-class classification problems. It is designed to produce a probability distribution over multiple classes, where the output values sum up to 1. Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f432ce52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12 (48.00 Byte)\n",
      "Trainable params: 12 (48.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "softmax output:\n",
      "[[0.7377959  0.2034388  0.05876529]\n",
      " [0.34991473 0.6424656  0.00761965]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#define input data\n",
    "input_data=[[0.5,1.2,0.8],[2.1,0.3,1.5]]\n",
    "\n",
    "#create a sequential model\n",
    "model=tf.keras.Sequential()\n",
    "\n",
    "#Add a dense layer without activation function\n",
    "model.add(tf.keras.layers.Dense(3,input_shape=(3,)))\n",
    "\n",
    "#Apply softmax activation function\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "#print model summary\n",
    "model.summary()\n",
    "\n",
    "#compute softmax output\n",
    "softmax_output=model.predict(input_data)\n",
    "\n",
    "print(\"softmax output:\")\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dbfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
