{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e677e3",
   "metadata": {},
   "source": [
    "# complete loss function\n",
    "loss function are an integral part of deep learning models as they quantity between predicted and actual values.\n",
    "\n",
    "(1) Mean Squared Error(MSE) loss:-\n",
    "definition:- calaulates the average squared difference between the predicted and target values\n",
    "\n",
    "Formula: MSE = (1/n) E(y_true-y_pred)^2, where n is the number of samples.\n",
    "\n",
    "Suitable for: Regression tasks where the target values are continuous.\n",
    "\n",
    "(2) Binary Cross-Entropy Loss:\n",
    "\n",
    "Definition: Measures the dissimilarity between two probability distributions (binary classification).\n",
    "\n",
    "Formula: BCE=-(y_true log(y_pred)+(1-y_true) log(1-y pred)), where y_true and y_pred are the true and predicted probabilities, respectively.\n",
    "\n",
    "Suitable for: Binary classification problems.\n",
    "\n",
    "(3) categorical cross- Entropy Loss:-\n",
    "definition:- calculation the cross- entropy loss for multi-class classification problems\n",
    "\n",
    "formula:- CCE=Σ(y_true*log(y_pred)+(1-y_true)*log(1-y_pred)),where y_true and y_pred are the true and predicited probabilities respectiviely\n",
    "\n",
    "Suitable for:- multi-class classification tasks.\n",
    "\n",
    "4). Sparse Categorical Cross-Entropy Loss:\n",
    "\n",
    "Definition. Similar to categorical cross-entropy, but the true labels are given as integers instead of one-hot encoded vectors.\n",
    "\n",
    "Formula: SCCE = -2(log(y_pred[true_label])), where true label represents the integer class label\n",
    "\n",
    "Suitable for: Multi-class classification tasks with integer-encoded labels.\n",
    "\n",
    "(5). Kullback-Leibler Divergence (KL Divergence) Loss:\n",
    "\n",
    "Definition: Measures the difference between two probability distributions.\n",
    "\n",
    "Formula: KL = Σ(y_true log(y_true/y_pred)), where y_true and y_pred are the true and predicted probability distributions, respectively.\n",
    "\n",
    "Suitable for. Tasks involving probabilistic models or when matching two distributions.\n",
    "\n",
    "(6). Huber Loss:\n",
    "\n",
    "Definition: Combines the characteristics of both MSE and MAE (Mean Absolute Error) to provide robustness against outliers.\n",
    "\n",
    "Formula: Huber = (1/n)*Σ(L), where L=0.5 (y_true-y_pred)^2 if ly true-y_pred <= o, and L=6ly true-y_pred] -0.5 62 otherwise, where ō is a threshold value.\n",
    "\n",
    "Suitable for: Regression tasks with outliers in the target data.\n",
    "\n",
    "(7) Mean Absolute Error(MAE) Loss:-\n",
    "\n",
    "Definition: Calculates the average absolute dimerence between the predicted and target values\n",
    "\n",
    "Formula: MAE = (1/n) Σly_pred - y_truel, where n is the number of samples.\n",
    "\n",
    "Suitable for: Regression tasks where the target values are continuous.\n",
    "\n",
    "(8). Hinge Loss (SVM Loss);\n",
    "\n",
    "Definition: Used in support vector machines (SVMs) and measures the margin of classification error.\n",
    "\n",
    "Formula: Hinge = Σmax(0, 1-y_true y pred), where y true and y pred are the true and predicted class labels, respectively.\n",
    "\n",
    "Suitable for: Binary classification tasks where SVMs or linear classifiers are employed\n",
    "\n",
    "(9). Sigmoid Cross-Entropy Loss.\n",
    "\n",
    "Definition: Combines the sigmoid activation function and binary cross-entropy loss.\n",
    "\n",
    "Formula: Sigmoid_CE=-Σ(y_true log(sigmoid(y_pred)) + (1-y_true) log(1-sigmoid(y_pred))), where y_true and y_pred are the true labels and predicted logits, respectively.\n",
    "\n",
    "Suitable for: Binary classification problems when using a sigmoid activation function.\n",
    "\n",
    "(10). Focal Loss:\n",
    "\n",
    "Definition: A variant of cross-entropy loss that addresses class imbalance by downweighting well-classified examples\n",
    "\n",
    "Formula: FL = Σ((1-y_pred)^yy_true\" log(y_pred)), where y_true and y pred are the true labels and predicted pr focusing parameter\n",
    "\n",
    "Suitable for: Imbalanced classification tasks where the majority class dominates\n",
    "\n",
    "(11). Wasserstein Loss (Earth Mover's Distance)\n",
    "\n",
    "Definition. Used in generative adversarial networks (GANS) to measure the dissimilarity between the generated and\n",
    "\n",
    "Formula: W = Σ(D(G(z)) - D(x)), where D represents the discriminator, G represents the generator, z represents the samples.\n",
    "\n",
    "suitable for:-training GANs and leraning the generator and discriminator models.\n",
    "\n",
    "(12) Triplet Loss:-\n",
    "\n",
    "Definition:- used in metrics learning tasks to encourage the embeding of smiliar samples to be closer while pushing dissimilar samples father apart.\n",
    "\n",
    "fromula:- \n",
    "\n",
    "\n",
    "suitable for:- face recognition , image retrival and other tasks where learning similarity metrics is important\n",
    "\n",
    "\n",
    "(13) Dice Loss(Sorensen- Dice Loss):-\n",
    "\n",
    "definition:- evaluates the overlap between predicted and true regions, often used in segmentation tasks.\n",
    "\n",
    "formula:-\n",
    "\n",
    "\n",
    "suitable for:- image segmentation problems\n",
    "\n",
    "(14) Contrastive Loss:-\n",
    "\n",
    "definition:- Encourages similar samples to have embeddings while pushing dissimilar samples apart in a learned embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
