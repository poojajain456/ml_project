{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d438c3",
   "metadata": {},
   "source": [
    "# Stress detection is a challenging task, as there are so many words that can be used by people on thoer posts that can show whether a person is habing psychological stress or not. while looking for datasets that i can use to train a machine learning model for stree detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8362986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f6afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\pujaj\\\\Downloads\\\\ML Data\\\\stress.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f103d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sentence_range</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>social_timestamp</th>\n",
       "      <th>social_karma</th>\n",
       "      <th>syntax_ari</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_dal_min_pleasantness</th>\n",
       "      <th>lex_dal_min_activation</th>\n",
       "      <th>lex_dal_min_imagery</th>\n",
       "      <th>lex_dal_avg_activation</th>\n",
       "      <th>lex_dal_avg_imagery</th>\n",
       "      <th>lex_dal_avg_pleasantness</th>\n",
       "      <th>social_upvote_ratio</th>\n",
       "      <th>social_num_comments</th>\n",
       "      <th>syntax_fk_grade</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>896</td>\n",
       "      <td>relationships</td>\n",
       "      <td>7nu7as</td>\n",
       "      <td>[50, 55]</td>\n",
       "      <td>Its like that, if you want or not.“ ME: I have...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.514981e+09</td>\n",
       "      <td>22</td>\n",
       "      <td>-1.238793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.65864</td>\n",
       "      <td>1.32245</td>\n",
       "      <td>1.80264</td>\n",
       "      <td>0.63</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.148707</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19059</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>680i6d</td>\n",
       "      <td>(5, 10)</td>\n",
       "      <td>I man the front desk and my title is HR Custom...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.493348e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>7.684583</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.69133</td>\n",
       "      <td>1.69180</td>\n",
       "      <td>1.97249</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>7.398222</td>\n",
       "      <td>-0.065909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7977</td>\n",
       "      <td>ptsd</td>\n",
       "      <td>8eeu1t</td>\n",
       "      <td>(5, 10)</td>\n",
       "      <td>We'd be saving so much money with this new hou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.524517e+09</td>\n",
       "      <td>10</td>\n",
       "      <td>2.360408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.70974</td>\n",
       "      <td>1.52985</td>\n",
       "      <td>1.86108</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "      <td>3.149288</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214</td>\n",
       "      <td>ptsd</td>\n",
       "      <td>8d28vu</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>My ex used to shoot back with \"Do you want me ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.524018e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>5.997000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.72615</td>\n",
       "      <td>1.52000</td>\n",
       "      <td>1.84909</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7</td>\n",
       "      <td>6.606000</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965</td>\n",
       "      <td>relationships</td>\n",
       "      <td>7r1e85</td>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>I haven’t said anything to him yet because I’m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.516200e+09</td>\n",
       "      <td>138</td>\n",
       "      <td>4.649418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75642</td>\n",
       "      <td>1.43582</td>\n",
       "      <td>1.91725</td>\n",
       "      <td>0.84</td>\n",
       "      <td>70</td>\n",
       "      <td>4.801869</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      subreddit post_id sentence_range  \\\n",
       "0    896  relationships  7nu7as       [50, 55]   \n",
       "1  19059        anxiety  680i6d        (5, 10)   \n",
       "2   7977           ptsd  8eeu1t        (5, 10)   \n",
       "3   1214           ptsd  8d28vu         [2, 7]   \n",
       "4   1965  relationships  7r1e85       [23, 28]   \n",
       "\n",
       "                                                text  label  confidence  \\\n",
       "0  Its like that, if you want or not.“ ME: I have...      0         0.8   \n",
       "1  I man the front desk and my title is HR Custom...      0         1.0   \n",
       "2  We'd be saving so much money with this new hou...      1         1.0   \n",
       "3  My ex used to shoot back with \"Do you want me ...      1         0.5   \n",
       "4  I haven’t said anything to him yet because I’m...      0         0.8   \n",
       "\n",
       "   social_timestamp  social_karma  syntax_ari  ...  lex_dal_min_pleasantness  \\\n",
       "0      1.514981e+09            22   -1.238793  ...                    1.0000   \n",
       "1      1.493348e+09             5    7.684583  ...                    1.4000   \n",
       "2      1.524517e+09            10    2.360408  ...                    1.1429   \n",
       "3      1.524018e+09             5    5.997000  ...                    1.0000   \n",
       "4      1.516200e+09           138    4.649418  ...                    1.1250   \n",
       "\n",
       "   lex_dal_min_activation  lex_dal_min_imagery  lex_dal_avg_activation  \\\n",
       "0                  1.2000                  1.0                 1.65864   \n",
       "1                  1.1250                  1.0                 1.69133   \n",
       "2                  1.0000                  1.0                 1.70974   \n",
       "3                  1.3000                  1.0                 1.72615   \n",
       "4                  1.1429                  1.0                 1.75642   \n",
       "\n",
       "   lex_dal_avg_imagery  lex_dal_avg_pleasantness  social_upvote_ratio  \\\n",
       "0              1.32245                   1.80264                 0.63   \n",
       "1              1.69180                   1.97249                 1.00   \n",
       "2              1.52985                   1.86108                 1.00   \n",
       "3              1.52000                   1.84909                 1.00   \n",
       "4              1.43582                   1.91725                 0.84   \n",
       "\n",
       "   social_num_comments  syntax_fk_grade  sentiment  \n",
       "0                   62        -0.148707   0.000000  \n",
       "1                    2         7.398222  -0.065909  \n",
       "2                    8         3.149288  -0.036818  \n",
       "3                    7         6.606000  -0.066667  \n",
       "4                   70         4.801869   0.141667  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe35f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Columns: 116 entries, id to sentiment\n",
      "dtypes: float64(107), int64(5), object(4)\n",
      "memory usage: 648.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699a4485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "subreddit                   0\n",
       "post_id                     0\n",
       "sentence_range              0\n",
       "text                        0\n",
       "                           ..\n",
       "lex_dal_avg_pleasantness    0\n",
       "social_upvote_ratio         0\n",
       "social_num_comments         0\n",
       "syntax_fk_grade             0\n",
       "sentiment                   0\n",
       "Length: 116, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e3e3b",
   "metadata": {},
   "source": [
    "# So this dataset does not have any null values. Now let's prepare the text column of this dataset to clean the text column with stopwords, links, special symbols and language errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70dae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pujaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stemmer=nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626f544b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 25\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mclean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[?]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m----> 8\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<?>+\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[s]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m re\u001b[38;5;241m.\u001b[39mescape(string\u001b[38;5;241m.\u001b[39mpunctuation), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m ,text)\n",
      "\u001b[1;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "def clean (text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    text = re.sub('\\[?]', '', text)\n",
    "\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    text = re.sub('<?>+', text)\n",
    "\n",
    "    text = re.sub('[s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    text = re.sub('\\n','' ,text)\n",
    "\n",
    "    text = re.sub('\\w\\d\\w', '',text)\n",
    "\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "\n",
    "    text=\"\".join(text)\n",
    "\n",
    "    text = [stemmer.stem (word) for word in text.split(' ')]\n",
    "\n",
    "    text=\"\".join(text)\n",
    "\n",
    "    return text\n",
    "df[\"text\"] = df[\"text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99736ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Obtaining dependency information for wordcloud from https://files.pythonhosted.org/packages/34/ac/72a4e42e76bf549dfd91791a6b10a9832f046c1d48b5e778be9ec012aa47/wordcloud-1.9.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wordcloud-1.9.2-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pujaj\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.2-cp311-cp311-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/151.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/151.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/151.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/151.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/151.4 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 30.7/151.4 kB 186.2 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 30.7/151.4 kB 186.2 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 30.7/151.4 kB 186.2 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 30.7/151.4 kB 186.2 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 41.0/151.4 kB 103.4 kB/s eta 0:00:02\n",
      "   ---------- ---------------------------- 41.0/151.4 kB 103.4 kB/s eta 0:00:02\n",
      "   --------------- ----------------------- 61.4/151.4 kB 121.3 kB/s eta 0:00:01\n",
      "   --------------- ----------------------- 61.4/151.4 kB 121.3 kB/s eta 0:00:01\n",
      "   --------------------- ----------------- 81.9/151.4 kB 143.4 kB/s eta 0:00:01\n",
      "   --------------------- ----------------- 81.9/151.4 kB 143.4 kB/s eta 0:00:01\n",
      "   ----------------------- --------------- 92.2/151.4 kB 137.9 kB/s eta 0:00:01\n",
      "   ----------------------- --------------- 92.2/151.4 kB 137.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 122.9/151.4 kB 156.7 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 122.9/151.4 kB 156.7 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 122.9/151.4 kB 156.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 151.4/151.4 kB 167.3 kB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WorldCloud,STOPWORDS,ImageColorGenerator\n",
    "text=\" \".join(i for i in df.text)\n",
    "stopwords=set(STOPWORDS)\n",
    "wordcloud=WordCloud(stopwords=stopwords,\n",
    "                   background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis=(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc57c90",
   "metadata": {},
   "source": [
    "# The label column in this dataset contains labels as 0 and 1.0 means no stress, and 1 means stress. I will use Stress and No stress labels instead of 1 and 0. So let's prepare this column accordingly and select the text and label columns for the process of training a machine learning model: ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2eb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  Its like that, if you want or not.“ ME: I have...  No Stree\n",
      "1  I man the front desk and my title is HR Custom...  No Stree\n",
      "2  We'd be saving so much money with this new hou...    Stress\n",
      "3  My ex used to shoot back with \"Do you want me ...    Stress\n",
      "4  I haven’t said anything to him yet because I’m...  No Stree\n"
     ]
    }
   ],
   "source": [
    "df['label']=df['label'].map({0:\"No Stree\", 1: \"Stress\"})\n",
    "df=df[[\"text\",\"label\"]]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6036ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x=np.array(df[\"text\"])\n",
    "y=np.array(df[\"label\"])\n",
    "\n",
    "cv=CountVectorizer()\n",
    "X=cv.fit_transform(x)\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y,\n",
    "                                          test_size=0.33,\n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdf710",
   "metadata": {},
   "source": [
    "# as this task is based on the problem of binary classification, i will be using the bernouil navie bayes algorithm, which is one of the best algorithms for binary classificayion problem  let's train the stress detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a23ce965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model=BernoulliNB()\n",
    "model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e8fa5",
   "metadata": {},
   "source": [
    "# now let's the performance of our model on some random sentences based on mental health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f396eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a textMy ex used to shoot back with \"Do you want me\n",
      "['No Stree']\n"
     ]
    }
   ],
   "source": [
    "user=input(\"enter a text\")\n",
    "data=cv.transform([user]).toarray()\n",
    "output=model.predict(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c8655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
